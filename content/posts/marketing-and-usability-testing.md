+++
linkTitle = 'Marketing and Usability Testing'
title = 'Marketing and Usability Testing'
shortTitle = 'Marketing and Usability Testing'
date = 2025-09-30T20:40:57-06:00
genres = ['other']
draft = false
+++

I recently began a virtual program called Founder University. It's startup school for people with ideas. To get in, my friend and I submitted our idea, which was to make software for scientists. They invited us for an interview, and then selected us for the program! 

The program started 2 weeks ago. I was on the other side of the world for the first two weeks, so I'm now catching up. I rewatched a session from Week 2, where someone gave a talk that was called, "Test, Build, Launch". What followed was some adjacent advice, at best, about how to become a content creator. I didn't find much of this session useful, but here are a few things I did get out of it.

One useful note I got out of the session is the presence of Ad Libraries for many social media platforms, like [Reddit](https://ads.reddit.com/inspiration), [Facebook](https://www.facebook.com/ads/library/), [Snapchat](https://www.snap.com/political-ads), [TikTok](https://ads.tiktok.com/business/creativecenter/inspiration/topads/), and my current employer, [Google](https://adstransparency.google.com/?region=US)[^google]. I searched for our competitors on these platforms for any ads they may be running, but didn't find anything. It makes me wonder if they are just behind the times, or have figured out that social media advertising doesn't work in their particular industry. I am highly skeptical that the latter is true, given that it has held true in every other industry in which social media ads have been tried in[^ads]. So this is some low-hanging fruit we may be able to exploit.

Another idea I got from the session is the concept of "loss-leader" software. Publish free tools which your customer base would find useful, and positively associate with your brand. However, the presenter then suggested that these can be cheaply and quickly made, and that it was okay if they were not very good. In my opinion that defeats the point - customers are not impressed that you can shoot out some code into a website, they are impressed if you make something they need. Bonus points if it's good. I happen to like extra credit a lot.

--

I'm back home for Week 3. This Monday's session was much better. It was about usability testing, a topic I'm familiar with because I've worked with UX designers before. But it's not a subject that I'm deeply knowledgable about[^sbir]. The presenter spoke in a rational manner about the goals of usability testing - how a good user experience should feel seamless for the user. The gold standard for usability is for the user to not need any instructions to accomplish the main workflows in a system. I agree with this sentiment[^caveat]. 

The presenter then selected a participant and went through a real example of usability testing for a website, by interviewing them about Founder University's application website. They began with a script that is read to the participant. 

This suddenly made things click for me - usability testing is like running a scientific experiment. This script could have come from a psychology clinical trial. The goal was to communicate the participant's rights, establish psychological safety, and reduce as many biases as possible.

What followed was a full usability testing session, lasting more than twenty minutes. I appreciated seeing this unabridged session in action. I have only heard of these sessions from designers, so every moment of it was fascinating to me. 

I appreciated the way that the presenter crafted questions with minimal to no inherent bias. When the participant made a comment while navigating the website, the presenter would ask a question like, "did this behavior meet your expectations?". This question is open-ended and allows the participant to answer in any number of ways. The question wasn't primed to make the participant answer in a particular way. Asking about a special new feature in an enthusiastic manner, on the other hand, might cow the participant to answer positively, as they can sense enthusiasm for the product.

That said, getting unbiased feedback is really hard, and I don't think that's the goal of usability testing. It's better to have a lot of imperfect data points, and then let the key decision makers "bend" a little bias of their own back into the results, in order to decide what to focus on next. It's okay if the results are somewhat biased - as long as they point in the right direction.

[^yuck]: I agree with actress Emma Thompson, who once derided the use of the word "content" to describe the cinema she has performed in, "...makes me feel like the stuffing inside a sofa cushion." Social media artifacts, unlike cinema, probably really is the only "content" out there. So the usage of the term is correct here.
[^google]: On contract with my consulting agency
[^ads]: I make this claim with 0 evidence whatsoever. It's hyperbole, but also feels like it has some truth.
[^sbir]: I was forced to learn more about usability testing recently while writing an application for R43 SBIR grant from the NIH. An unexpectedly good resource for understanding usability testing comes from Microsoft's ["Getting Started Developing User Interfaces for Windows Applications"](https://learn.microsoft.com/pdf?url=https%3A%2F%2Flearn.microsoft.com%2Fen-us%2Fwindows%2Fwin32%2Fappuistart%2Ftoc.json).
[^caveat]: Complicated actions sometimes require instructions, which is okay. The canonical examples in UX come from the design of fighter jet cockpits - while their use requires many hours of training, the mental model of operation is consistent. The goal is to prevent highly-experienced pilots from making simple errors while in the cockpit and under duress. Most of us aren't defining anything close to a fighter jet cockpit, so it's really worth asking whether your animal colony management system requires detailed instructions in order to move mice between cages and record experimental data.
